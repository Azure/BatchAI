{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This example uses the MNIST dataset to demonstrate how to train a convolutional neural network (LeNet) on a GPU cluster. This recipe is on a single node.\n",
    "\n",
    "## Details\n",
    "\n",
    "- For demonstration purposes, MNIST dataset and Caffe configuration file will be deployed at Azure File Share;\n",
    "- Standard output of the job and the model will be stored on Azure File Share;\n",
    "- MNIST dataset has been preprocessed according to http://caffe.berkeleyvision.org/gathered/examples/mnist.html available at https://batchaisamples.blob.core.windows.net/samples/mnist_lmdb.zip?st=2017-10-06T00%3A15%3A00Z&se=2100-01-01T00%3A00%3A00Z&sp=rl&sv=2016-05-31&sr=b&sig=jKlQA8x190lLGDXloeHrSe6jpOtUEYLD1DRoyWuiAdQ%3D \n",
    "- The original Caffe solver and net prototxt files have been modified to take environment variables: AZ_BATCHAI_INPUT_SAMPLE and AZ_BATCHAI_OUTPUT_MODEL, and available here [lenet_solver.prototxt](./lenet_solver.prototxt) and [lenet_train_test.prototxt](./lenet_train_test.prototxt). \n",
    "- Since prototxt files supports neither command line overloading nor environment variable, we use job preparation task [preparation_script.sh](./preparation_script.sh) to expand the environment variable specified in the files, providing more flexibility of the job setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Install Dependencies and Create Configuration file.\n",
    "Follow [instructions](/recipes) to install all dependencies and create configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration and Create Batch AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "bfa11f00-8866-4051-bbfe-a9646e004910"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "from azure.storage.file import FileService\n",
    "from azure.storage.blob import BlockBlobService\n",
    "import azure.mgmt.batchai.models as models\n",
    "\n",
    "# utilities.py contains helper functions used by different notebooks\n",
    "sys.path.append('../..')\n",
    "import utilities\n",
    "\n",
    "cfg = utilities.Configuration('../../configuration.json')\n",
    "client = utilities.create_batchai_client(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Resoruce Group and Batch AI workspace if not exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utilities.create_resource_group(cfg)\n",
    "_ = client.workspaces.create(cfg.resource_group, cfg.workspace, cfg.location).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Training Dataset and Script in Azure Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure Blob Container\n",
    "\n",
    "We will create a new Blob Container with name `batchaisample` under your storage account. This will be used to store the *input training dataset*\n",
    "\n",
    "**Note** You don't need to create new blob Container for every cluster. We are doing this in this sample to simplify resource management for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_blob_container_name = 'batchaisample'\n",
    "blob_service = BlockBlobService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "blob_service.create_container(azure_blob_container_name, fail_on_exist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload MNIST Dataset to Azure Blob Container\n",
    "\n",
    "For demonstration purposes, we will download preprocessed MNIST dataset to the current directory and upload it to Azure Blob Container directory named `mnist_dataset`.\n",
    "\n",
    "There are multiple ways to create folders and upload files into Azure Blob Container - you can use [Azure Portal](https://ms.portal.azure.com), [Storage Explorer](http://storageexplorer.com/), [Azure CLI2](/azure-cli-extension) or Azure SDK for your preferable programming language.\n",
    "In this example we will use Azure SDK for python to copy files into Blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://batchaisamples.blob.core.windows.net/samples/mnist_dataset_full.zip?st=2018-03-04T00%3A21%3A00Z&se=2099-12-31T23%3A59%3A00Z&sp=rl&sv=2017-04-17&sr=b&sig=rrBgTFeIv3bjsyAfh87RoW5i0ay4mMyMEIh2RI45s%2B0%3D ...Done\n",
      "Extracting MNIST dataset...\n",
      "Uploading MNIST dataset...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset_directory = 'mnist_dataset'\n",
    "utilities.download_and_upload_mnist_dataset_to_blob(\n",
    "    blob_service, azure_blob_container_name, mnist_dataset_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure File Share\n",
    "\n",
    "For this example we will create a new File Share with name `batchaisample` under your storage account. This will be used to share the *training script file* and *output file*.\n",
    "\n",
    "**Note** You don't need to create new file share for every cluster. We are doing this in this sample to simplify resource management for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_file_share_name = 'batchaisample'\n",
    "file_service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "file_service.create_share(azure_file_share_name, fail_on_exist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Sample Script to Azure File Share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe_sample_directory = 'caffemnistsample'\n",
    "file_service.create_directory(\n",
    "    azure_file_share_name, caffe_sample_directory, fail_on_exist=False)\n",
    "file_service.create_file_from_path(\n",
    "    azure_file_share_name, caffe_sample_directory, 'lenet_solver.prototxt.template', 'lenet_solver.prototxt')\n",
    "file_service.create_file_from_path(\n",
    "    azure_file_share_name, caffe_sample_directory, 'lenet_train_test.prototxt.template', 'lenet_train_test.prototxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to upload the preparation script [preparation_script.sh](./preparation_script.sh). It will expand the environment variable used specified inside lenet_solver.prototxt and lenet_train_test.prototxt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_service.create_file_from_path(\n",
    "    azure_file_share_name, caffe_sample_directory, 'preparation_script.sh', 'preparation_script.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Azure Batch AI Compute Cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Compute Cluster\n",
    "\n",
    "- For this example we will use a GPU cluster of `STANDARD_NC6` nodes. Number of nodes in the cluster is configured with `nodes_count` variable;\n",
    "- We will call the cluster `nc6`;\n",
    "\n",
    "So, the cluster will have the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes_count = 1\n",
    "cluster_name = 'nc6'\n",
    "\n",
    "parameters = models.ClusterCreateParameters(\n",
    "    location=cfg.location,\n",
    "    vm_size='STANDARD_NC6',\n",
    "    scale_settings=models.ScaleSettings(\n",
    "        manual=models.ManualScaleSettings(target_node_count=nodes_count)\n",
    "    ),\n",
    "    user_account_settings=models.UserAccountSettings(\n",
    "        admin_user_name=cfg.admin,\n",
    "        admin_user_password=cfg.admin_password or None,\n",
    "        admin_user_ssh_public_key=cfg.admin_ssh_key or None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = client.clusters.create(cfg.resource_group, cfg.workspace, cluster_name, parameters).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Cluster Creation\n",
    "\n",
    "Monitor the just created cluster. utilities.py contains a helper function to print out detail status of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster state: steady Target: 2; Allocated: 2; Idle: 2; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n"
     ]
    }
   ],
   "source": [
    "cluster = client.clusters.get(cfg.resource_group, cfg.workspace, cluster_name)\n",
    "utilities.print_cluster_status(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Azure Batch AI Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Job\n",
    "- The job will use `bvlc/caffe:gpu` container.\n",
    "- Will use configured previously input and output directories;\n",
    "- Will run modified lenet_solver.prototxt without any additional command line arguement.\n",
    "- Will mount file share at folder with name `afs`. Full path of this folder on a computer node will be `$AZ_BATCHAI_JOB_MOUNT_ROOT/afs`;\n",
    "- Will mount Azure Blob Container at folder with name `bfs`. Full path of this folder on a computer node will be `$AZ_BATCHAI_JOB_MOUNT_ROOT/bfs`;\n",
    "- The job needs to know where to find mnist_replica.py and input MNIST dataset. We will create two input directories for this. The job will be able to reference those directories using environment variables:\n",
    "    - ```AZ_BATCHAI_INPUT_SCRIPT``` : refers to the directory containing the scripts at mounted Azure File Share \n",
    "    - ```AZ_BATCHAI_INPUT_DATASET``` : refers to the directory containing the training data on mounted Azure Blob Container\n",
    "- The job needs to know where to find the uploaded scripts and input MNIST dataset. The job will be able to reference those directories using ```AZ_BATCHAI_INPUT_SAMPLE```environment variable.\n",
    "- The model output will be stored in File Share.\n",
    "- The job will be able to reference this directory as `$AZ_BATCHAI_OUTPUT_MODEL` and we will be able to enumerate files in this directory using `MODEL` id.\n",
    "- Unlike CNTK BrainScript, the caffe solver config file in Caffe cannot be overloaded, which means paths to network file and dataset cannot be modified outside the prototxt file. In this case, we use job preparation task directly expand the environment varibales in prototxt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "azure_file_share = 'afs'\n",
    "azure_blob = 'bfs'\n",
    "parameters = models.JobCreateParameters(\n",
    "     location=cfg.location,\n",
    "     cluster=models.ResourceId(id=cluster.id),\n",
    "     node_count=1,\n",
    "     mount_volumes=models.MountVolumes(\n",
    "            azure_file_shares=[\n",
    "                models.AzureFileShareReference(\n",
    "                    account_name=cfg.storage_account_name,\n",
    "                    credentials=models.AzureStorageCredentialsInfo(\n",
    "                        account_key=cfg.storage_account_key),\n",
    "                    azure_file_url='https://{0}.file.core.windows.net/{1}'.format(\n",
    "                        cfg.storage_account_name, azure_file_share_name),\n",
    "                    relative_mount_path=azure_file_share)\n",
    "            ],\n",
    "            azure_blob_file_systems=[\n",
    "                models.AzureBlobFileSystemReference(\n",
    "                    account_name=cfg.storage_account_name,\n",
    "                    credentials=models.AzureStorageCredentialsInfo(\n",
    "                        account_key=cfg.storage_account_key),\n",
    "                    container_name=azure_blob_container_name,\n",
    "                    relative_mount_path=azure_blob)\n",
    "            ]\n",
    "        ),\n",
    "     input_directories = [\n",
    "         models.InputDirectory(\n",
    "            id='SAMPLE',\n",
    "            path='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}/{1}'.format(azure_file_share, caffe_sample_directory)),\n",
    "         models.InputDirectory(\n",
    "            id='DATA',\n",
    "            path='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}/{1}'.format(azure_blob, mnist_dataset_directory))],\n",
    "     std_out_err_path_prefix='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}'.format(azure_file_share),\n",
    "     output_directories = [\n",
    "        models.OutputDirectory(\n",
    "            id='MODEL',\n",
    "            path_prefix='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}'.format(azure_file_share),\n",
    "            path_suffix='Models')],\n",
    "     job_preparation=models.JobPreparation(command_line=\"bash $AZ_BATCHAI_INPUT_SAMPLE/preparation_script.sh\"),\n",
    "     container_settings=models.ContainerSettings(\n",
    "         image_source_registry=models.ImageSourceRegistry(image='bvlc/caffe:gpu')),\n",
    "     caffe_settings = models.CaffeSettings(\n",
    "         config_file_path='$AZ_BATCHAI_INPUT_SAMPLE/lenet_solver.prototxt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training Job and wait for Job completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Job caffe_06_08_2018_012509 in Experiment caffe_experiment\n"
     ]
    }
   ],
   "source": [
    "experiment_name = 'caffe_experiment'\n",
    "experiment = client.experiments.create(cfg.resource_group, cfg.workspace, experiment_name).result()\n",
    "job_name = datetime.utcnow().strftime('caffe_%m_%d_%Y_%H%M%S')\n",
    "job = client.jobs.create(cfg.resource_group, cfg.workspace, experiment_name, job_name, parameters).result()\n",
    "print('Created Job {0} in Experiment {1}'.format(job.name, experiment.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Job to Finish\n",
    "The job will start running when the cluster will have enough idle nodes. The following code waits for job to start running printing the cluster state. During job run, the code prints current content of stderr.txt\n",
    "\n",
    "**Note** Execution may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster state: steady Target: 2; Allocated: 2; Idle: 1; Unusable: 0; Running: 1; Preparing: 0; Leaving: 0\n",
      "Job state: running ExitCode: None\n",
      "Waiting for job output to become available...\n",
      "I0608 01:25:26.635673   551 caffe.cpp:218] Using GPUs 0\n",
      "I0608 01:25:26.652570   551 caffe.cpp:223] GPU 0: Tesla K80\n",
      "I0608 01:25:27.059180   551 solver.cpp:44] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.01\n",
      "display: 100\n",
      "max_iter: 10000\n",
      "lr_policy: \"inv\"\n",
      "gamma: 0.0001\n",
      "power: 0.75\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "snapshot: 5000\n",
      "snapshot_prefix: \"/mnt/batch/tasks/shared/LS_root/jobs/pgunda/caffe_experiment/caffe_06_08_2018_012509/mounts/afs/1cba1da6-5a83-45e1-a88e-8b397eb84356/batchaitestrgnortheurope/workspaces/pgunda/experiments/caffe_experiment/jobs/caffe_06_08_2018_012509/21459a0a-cec0-4935-891e-dfd6ec1e4be0/outputs/Models/lenet\"\n",
      "solver_mode: GPU\n",
      "device_id: 0\n",
      "net: \"/mnt/batch/tasks/shared/LS_root/jobs/pgunda/caffe_experiment/caffe_06_08_2018_012509/mounts/afs/caffemnistsample/lenet_train_test.prototxt\"\n",
      "train_state {\n",
      "  level: 0\n",
      "  stage: \"\"\n",
      "}\n",
      "I0608 01:25:27.400308   551 solver.cpp:87] Creating training net from net file: /mnt/batch/tasks/shared/LS_root/jobs/pgunda/caffe_experiment/caffe_06_08_2018_012509/mounts/afs/caffemnistsample/lenet_train_test.prototxt\n",
      "I0608 01:25:27.456534   551 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist\n",
      "I0608 01:25:27.456567   551 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0608 01:25:27.456676   551 net.cpp:51] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "  level: 0\n",
      "  stage: \"\"\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"/mnt/batch/tasks/shared/LS_root/jobs/pgunda/caffe_experiment/caffe_06_08_2018_012509/mounts/bfs/mnist_dataset/mnist_train_lmdb\"\n",
      "    batch_size: 64\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0608 01:25:27.456743   551 layer_factory.hpp:77] Creating layer mnist\n",
      "I0608 01:25:28.392572   551 db_lmdb.cpp:35] Opened lmdb /mnt/batch/tasks/shared/LS_root/jobs/pgunda/caffe_experiment/caffe_06_08_2018_012509/mounts/bfs/mnist_dataset/mnist_train_lmdb\n",
      "I0608 01:25:28.392827   551 net.cpp:84] Creating Layer mnist\n",
      "I0608 01:25:28.392858   551 net.cpp:380] mnist -> data\n",
      "I0608 01:25:28.392890   551 net.cpp:380] mnist -> label\n",
      "I0608 01:25:28.393746   551 data_layer.cpp:45] output data size: 64,1,28,28\n",
      "I0608 01:25:28.395159   551 net.cpp:122] Setting up mnist\n",
      "I0608 01:25:28.395180   551 net.cpp:129] Top shape: 64 1 28 28 (50176)\n",
      "I0608 01:25:28.395195   551 net.cpp:129] Top shape: 64 (64)\n",
      "I0608 01:25:28.395236   551 net.cpp:137] Memory required for data: 200960\n",
      "I0608 01:25:28.395253   551 layer_factory.hpp:77] Creating layer conv1\n",
      "I0608 01:25:28.395288   551 net.cpp:84] Creating Layer conv1\n",
      "I0608 01:25:28.395300   551 net.cpp:406] conv1 <- data\n",
      "I0608 01:25:28.395319   551 net.cpp:380] conv1 -> conv1\n",
      "I0608 01:25:29.070338   551 net.cpp:122] Setting up conv1\n",
      "I0608 01:25:29.070374   551 net.cpp:129] Top shape: 64 20 24 24 (737280)\n",
      "I0608 01:25:29.070384   551 net.cpp:137] Memory required for data: 3150080\n",
      "I0608 01:25:29.070415   551 layer_factory.hpp:77] Creating layer pool1\n",
      "I0608 01:25:29.070439   551 net.cpp:84] Creating Layer pool1\n",
      "I0608 01:25:29.070449   551 net.cpp:406] pool1 <- conv1\n",
      "I0608 01:25:29.070461   551 net.cpp:380] pool1 -> pool1\n",
      "I0608 01:25:29.070531   551 net.cpp:122] Setting up pool1\n",
      "I0608 01:25:29.070544   551 net.cpp:129] Top shape: 64 20 12 12 (184320)\n",
      "I0608 01:25:29.070552   551 net.cpp:137] Memory required for data: 3887360\n",
      "I0608 01:25:29.070560   551 layer_factory.hpp:77] Creating layer conv2\n",
      "I0608 01:25:29.070580   551 net.cpp:84] Creating Layer conv2\n",
      "I0608 01:25:29.070590   551 net.cpp:406] conv2 <- pool1\n",
      "I0608 01:25:29.070603   551 net.cpp:380] conv2 -> conv2\n",
      "I0608 01:25:29.072245   551 net.cpp:122] Setting up conv2\n",
      "I0608 01:25:29.072264   551 net.cpp:129] Top shape: 64 50 8 8 (204800)\n",
      "I0608 01:25:29.072273   551 net.cpp:137] Memory required for data: 4706560\n",
      "I0608 01:25:29.072288   551 layer_factory.hpp:77] Creating layer pool2\n",
      "I0608 01:25:29.072305   551 net.cpp:84] Creating Layer pool2\n",
      "I0608 01:25:29.072314   551 net.cpp:406] pool2 <- conv2\n",
      "I0608 01:25:29.072329   551 net.cpp:380] pool2 -> pool2\n",
      "I0608 01:25:29.072379   551 net.cpp:122] Setting up pool2\n",
      "I0608 01:25:29.072391   551 net.cpp:129] Top shape: 64 50 4 4 (51200)\n",
      "I0608 01:25:29.072399   551 net.cpp:137] Memory required for data: 4911360\n",
      "I0608 01:25:29.072407   551 layer_factory.hpp:77] Creating layer ip1\n",
      "I0608 01:25:29.072424   551 net.cpp:84] Creating Layer ip1\n",
      "I0608 01:25:29.072433   551 net.cpp:406] ip1 <- pool2\n",
      "I0608 01:25:29.072444   551 net.cpp:380] ip1 -> ip1\n",
      "I0608 01:25:29.075430   551 net.cpp:122] Setting up ip1\n",
      "I0608 01:25:29.075454   551 net.cpp:129] Top shape: 64 500 (32000)\n",
      "I0608 01:25:29.075462   551 net.cpp:137] Memory required for data: 5039360\n",
      "I0608 01:25:29.075482   551 layer_factory.hpp:77] Creating layer relu1\n",
      "I0608 01:25:29.075496   551 net.cpp:84] Creating Layer relu1\n",
      "I0608 01:25:29.075505   551 net.cpp:406] relu1 <- ip1\n",
      "I0608 01:25:29.075516   551 net.cpp:367] relu1 -> ip1 (in-place)\n",
      "I0608 01:25:29.075984   551 net.cpp:122] Setting up relu1\n",
      "I0608 01:25:29.076001   551 net.cpp:129] Top shape: 64 500 (32000)\n",
      "I0608 01:25:29.076010   551 net.cpp:137] Memory required for data: 5167360\n",
      "I0608 01:25:29.076020   551 layer_factory.hpp:77] Creating layer ip2\n",
      "I0608 01:25:29.076036   551 net.cpp:84] Creating Layer ip2\n",
      "I0608 01:25:29.076045   551 net.cpp:406] ip2 <- ip1\n",
      "I0608 01:25:29.076057   551 net.cpp:380] ip2 -> ip2\n",
      "I0608 01:25:29.076831   551 net.cpp:122] Setting up ip2\n",
      "I0608 01:25:29.076848   551 net.cpp:129] Top shape: 64 10 (640)\n",
      "I0608 01:25:29.076856   551 net.cpp:137] Memory required for data: 5169920\n",
      "I0608 01:25:29.076869   551 layer_factory.hpp:77] Creating layer loss\n",
      "I0608 01:25:29.076885   551 net.cpp:84] Creating Layer loss\n",
      "I0608 01:25:29.076894   551 net.cpp:406] loss <- ip2\n",
      "I0608 01:25:29.076905   551 net.cpp:406] loss <- label\n",
      "I0608 01:25:29.076917   551 net.cpp:380] loss -> loss\n",
      "I0608 01:25:29.076944   551 layer_factory.hpp:77] Creating layer loss\n",
      "I0608 01:25:29.077317   551 net.cpp:122] Setting up loss\n",
      "I0608 01:25:29.077332   551 net.cpp:129] Top shape: (1)\n",
      "I0608 01:25:29.077342   551 net.cpp:132]     with loss weight 1\n",
      "I0608 01:25:29.077368   551 net.cpp:137] Memory required for data: 5169924\n",
      "I0608 01:25:29.077376   551 net.cpp:198] loss needs backward computation.\n",
      "I0608 01:25:29.077390   551 net.cpp:198] ip2 needs backward computation.\n",
      "I0608 01:25:29.077399   551 net.cpp:198] relu1 needs backward computation.\n",
      "I0608 01:25:29.077407   551 net.cpp:198] ip1 needs backward computation.\n",
      "I0608 01:25:29.077452   551 net.cpp:198] pool2 needs backward computation.\n",
      "I0608 01:25:29.077462   551 net.cpp:198] conv2 needs backward computation.\n",
      "I0608 01:25:29.077471   551 net.cpp:198] pool1 needs backward computation.\n",
      "I0608 01:25:29.077479   551 net.cpp:198] conv1 needs backward computation.\n",
      "I0608 01:25:29.077488   551 net.cpp:200] mnist does not need backward computation.\n",
      "I0608 01:25:29.077497   551 net.cpp:242] This network produces output loss\n",
      "I0608 01:25:29.077514   551 net.cpp:255] Network initialization done.\n",
      "I0608 01:25:29.135581   551 solver.cpp:172] Creating test net (#0) specified by net file: /mnt/batch/tasks/shared/LS_root/jobs/pgunda/caffe_experiment/caffe_06_08_2018_012509/mounts/afs/caffemnistsample/lenet_train_test.prototxt\n",
      "I0608 01:25:29.135635   551 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I0608 01:25:29.135733   551 net.cpp:51] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"/mnt/batch/tasks/shared/LS_root/jobs/pgunda/caffe_experiment/caffe_06_08_2018_012509/mounts/bfs/mnist_dataset/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0608 01:25:29.135818   551 layer_factory.hpp:77] Creating layer mnist\n",
      "I0608 01:25:29.482345   551 db_lmdb.cpp:35] Opened lmdb /mnt/batch/tasks/shared/LS_root/jobs/pgunda/caffe_experiment/caffe_06_08_2018_012509/mounts/bfs/mnist_dataset/mnist_test_lmdb\n",
      "I0608 01:25:29.482578   551 net.cpp:84] Creating Layer mnist\n",
      "I0608 01:25:29.482605   551 net.cpp:380] mnist -> data\n",
      "I0608 01:25:29.482627   551 net.cpp:380] mnist -> label\n",
      "I0608 01:25:29.482826   551 data_layer.cpp:45] output data size: 100,1,28,28\n",
      "I0608 01:25:29.485602   551 net.cpp:122] Setting up mnist\n",
      "I0608 01:25:29.485630   551 net.cpp:129] Top shape: 100 1 28 28 (78400)\n",
      "I0608 01:25:29.485640   551 net.cpp:129] Top shape: 100 (100)\n",
      "I0608 01:25:29.485647   551 net.cpp:137] Memory required for data: 314000\n",
      "I0608 01:25:29.485659   551 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I0608 01:25:29.485680   551 net.cpp:84] Creating Layer label_mnist_1_split\n",
      "I0608 01:25:29.485697   551 net.cpp:406] label_mnist_1_split <- label\n",
      "I0608 01:25:29.485736   551 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I0608 01:25:29.485756   551 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I0608 01:25:29.485816   551 net.cpp:122] Setting up label_mnist_1_split\n",
      "I0608 01:25:29.485828   551 net.cpp:129] Top shape: 100 (100)\n",
      "I0608 01:25:29.485837   551 net.cpp:129] Top shape: 100 (100)\n",
      "I0608 01:25:29.485846   551 net.cpp:137] Memory required for data: 314800\n",
      "I0608 01:25:29.485854   551 layer_factory.hpp:77] Creating layer conv1\n",
      "I0608 01:25:29.485875   551 net.cpp:84] Creating Layer conv1\n",
      "I0608 01:25:29.485884   551 net.cpp:406] conv1 <- data\n",
      "I0608 01:25:29.485899   551 net.cpp:380] conv1 -> conv1\n",
      "I0608 01:25:29.492278   551 net.cpp:122] Setting up conv1\n",
      "I0608 01:25:29.492307   551 net.cpp:129] Top shape: 100 20 24 24 (1152000)\n",
      "I0608 01:25:29.492317   551 net.cpp:137] Memory required for data: 4922800\n",
      "I0608 01:25:29.492336   551 layer_factory.hpp:77] Creating layer pool1\n",
      "I0608 01:25:29.492350   551 net.cpp:84] Creating Layer pool1\n",
      "I0608 01:25:29.492360   551 net.cpp:406] pool1 <- conv1\n",
      "I0608 01:25:29.492377   551 net.cpp:380] pool1 -> pool1\n",
      "I0608 01:25:29.492436   551 net.cpp:122] Setting up pool1\n",
      "I0608 01:25:29.492449   551 net.cpp:129] Top shape: 100 20 12 12 (288000)\n",
      "I0608 01:25:29.492457   551 net.cpp:137] Memory required for data: 6074800\n",
      "I0608 01:25:29.492465   551 layer_factory.hpp:77] Creating layer conv2\n",
      "I0608 01:25:29.492488   551 net.cpp:84] Creating Layer conv2\n",
      "I0608 01:25:29.492498   551 net.cpp:406] conv2 <- pool1\n",
      "I0608 01:25:29.492509   551 net.cpp:380] conv2 -> conv2\n",
      "I0608 01:25:29.494592   551 net.cpp:122] Setting up conv2\n",
      "I0608 01:25:29.494611   551 net.cpp:129] Top shape: 100 50 8 8 (320000)\n",
      "I0608 01:25:29.494621   551 net.cpp:137] Memory required for data: 7354800\n",
      "I0608 01:25:29.494635   551 layer_factory.hpp:77] Creating layer pool2\n",
      "I0608 01:25:29.494652   551 net.cpp:84] Creating Layer pool2\n",
      "I0608 01:25:29.494660   551 net.cpp:406] pool2 <- conv2\n",
      "I0608 01:25:29.494673   551 net.cpp:380] pool2 -> pool2\n",
      "I0608 01:25:29.495040   551 net.cpp:122] Setting up pool2\n",
      "I0608 01:25:29.495057   551 net.cpp:129] Top shape: 100 50 4 4 (80000)\n",
      "I0608 01:25:29.495065   551 net.cpp:137] Memory required for data: 7674800\n",
      "I0608 01:25:29.495074   551 layer_factory.hpp:77] Creating layer ip1\n",
      "I0608 01:25:29.495092   551 net.cpp:84] Creating Layer ip1\n",
      "I0608 01:25:29.495101   551 net.cpp:406] ip1 <- pool2\n",
      "I0608 01:25:29.495112   551 net.cpp:380] ip1 -> ip1\n",
      "I0608 01:25:29.498986   551 net.cpp:122] Setting up ip1\n",
      "I0608 01:25:29.499012   551 net.cpp:129] Top shape: 100 500 (50000)\n",
      "I0608 01:25:29.499022   551 net.cpp:137] Memory required for data: 7874800\n",
      "I0608 01:25:29.499037   551 layer_factory.hpp:77] Creating layer relu1\n",
      "I0608 01:25:29.499052   551 net.cpp:84] Creating Layer relu1\n",
      "I0608 01:25:29.499061   551 net.cpp:406] relu1 <- ip1\n",
      "I0608 01:25:29.499071   551 net.cpp:367] relu1 -> ip1 (in-place)\n",
      "I0608 01:25:29.499510   551 net.cpp:122] Setting up relu1\n",
      "I0608 01:25:29.499526   551 net.cpp:129] Top shape: 100 500 (50000)\n",
      "I0608 01:25:29.499533   551 net.cpp:137] Memory required for data: 8074800\n",
      "I0608 01:25:29.499542   551 layer_factory.hpp:77] Creating layer ip2\n",
      "I0608 01:25:29.499560   551 net.cpp:84] Creating Layer ip2\n",
      "I0608 01:25:29.499570   551 net.cpp:406] ip2 <- ip1\n",
      "I0608 01:25:29.499585   551 net.cpp:380] ip2 -> ip2\n",
      "I0608 01:25:29.499758   551 net.cpp:122] Setting up ip2\n",
      "I0608 01:25:29.499771   551 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0608 01:25:29.499779   551 net.cpp:137] Memory required for data: 8078800\n",
      "I0608 01:25:29.499790   551 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I0608 01:25:29.499802   551 net.cpp:84] Creating Layer ip2_ip2_0_split\n",
      "I0608 01:25:29.499811   551 net.cpp:406] ip2_ip2_0_split <- ip2\n",
      "I0608 01:25:29.499821   551 net.cpp:380] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I0608 01:25:29.499836   551 net.cpp:380] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I0608 01:25:29.499887   551 net.cpp:122] Setting up ip2_ip2_0_split\n",
      "I0608 01:25:29.499898   551 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0608 01:25:29.499912   551 net.cpp:129] Top shape: 100 10 (1000)\n",
      "I0608 01:25:29.499945   551 net.cpp:137] Memory required for data: 8086800\n",
      "I0608 01:25:29.499954   551 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0608 01:25:29.499970   551 net.cpp:84] Creating Layer accuracy\n",
      "I0608 01:25:29.499979   551 net.cpp:406] accuracy <- ip2_ip2_0_split_0\n",
      "I0608 01:25:29.499990   551 net.cpp:406] accuracy <- label_mnist_1_split_0\n",
      "I0608 01:25:29.500003   551 net.cpp:380] accuracy -> accuracy\n",
      "I0608 01:25:29.500017   551 net.cpp:122] Setting up accuracy\n",
      "I0608 01:25:29.500027   551 net.cpp:129] Top shape: (1)\n",
      "I0608 01:25:29.500036   551 net.cpp:137] Memory required for data: 8086804\n",
      "I0608 01:25:29.500043   551 layer_factory.hpp:77] Creating layer loss\n",
      "I0608 01:25:29.500054   551 net.cpp:84] Creating Layer loss\n",
      "I0608 01:25:29.500063   551 net.cpp:406] loss <- ip2_ip2_0_split_1\n",
      "I0608 01:25:29.500072   551 net.cpp:406] loss <- label_mnist_1_split_1\n",
      "I0608 01:25:29.500082   551 net.cpp:380] loss -> loss\n",
      "I0608 01:25:29.500095   551 layer_factory.hpp:77] Creating layer loss\n",
      "I0608 01:25:29.500445   551 net.cpp:122] Setting up loss\n",
      "I0608 01:25:29.500463   551 net.cpp:129] Top shape: (1)\n",
      "I0608 01:25:29.500470   551 net.cpp:132]     with loss weight 1\n",
      "I0608 01:25:29.500488   551 net.cpp:137] Memory required for data: 8086808\n",
      "I0608 01:25:29.500497   551 net.cpp:198] loss needs backward computation.\n",
      "I0608 01:25:29.500507   551 net.cpp:200] accuracy does not need backward computation.\n",
      "I0608 01:25:29.500516   551 net.cpp:198] ip2_ip2_0_split needs backward computation.\n",
      "I0608 01:25:29.500525   551 net.cpp:198] ip2 needs backward computation.\n",
      "I0608 01:25:29.500533   551 net.cpp:198] relu1 needs backward computation.\n",
      "I0608 01:25:29.500542   551 net.cpp:198] ip1 needs backward computation.\n",
      "I0608 01:25:29.500550   551 net.cpp:198] pool2 needs backward computation.\n",
      "I0608 01:25:29.500560   551 net.cpp:198] conv2 needs backward computation.\n",
      "I0608 01:25:29.500567   551 net.cpp:198] pool1 needs backward computation.\n",
      "I0608 01:25:29.500576   551 net.cpp:198] conv1 needs backward computation.\n",
      "I0608 01:25:29.500586   551 net.cpp:200] label_mnist_1_split does not need backward computation.\n",
      "I0608 01:25:29.500594   551 net.cpp:200] mnist does not need backward computation.\n",
      "I0608 01:25:29.500602   551 net.cpp:242] This network produces output accuracy\n",
      "I0608 01:25:29.500612   551 net.cpp:242] This network produces output loss\n",
      "I0608 01:25:29.500629   551 net.cpp:255] Network initialization done.\n",
      "I0608 01:25:29.500680   551 solver.cpp:56] Solver scaffolding done.\n",
      "I0608 01:25:29.500948   551 caffe.cpp:248] Starting Optimization\n",
      "I0608 01:25:29.500967   551 solver.cpp:272] Solving LeNet\n",
      "I0608 01:25:29.500974   551 solver.cpp:273] Learning Rate Policy: inv\n",
      "I0608 01:25:29.501516   551 solver.cpp:330] Iteration 0, Testing net (#0)\n",
      "I0608 01:25:29.704588   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:29.711800   551 solver.cpp:397]     Test net output #0: accuracy = 0.071\n",
      "I0608 01:25:29.711843   551 solver.cpp:397]     Test net output #1: loss = 2.42432 (* 1 = 2.42432 loss)\n",
      "I0608 01:25:29.717839   551 solver.cpp:218] Iteration 0 (3.17806e+09 iter/s, 0.216819s/100 iters), loss = 2.37471\n",
      "I0608 01:25:29.717883   551 solver.cpp:237]     Train net output #0: loss = 2.37471 (* 1 = 2.37471 loss)\n",
      "I0608 01:25:29.717905   551 sgd_solver.cpp:105] Iteration 0, lr = 0.01\n",
      "I0608 01:25:30.086784   551 solver.cpp:218] Iteration 100 (271.09 iter/s, 0.368881s/100 iters), loss = 0.224091\n",
      "I0608 01:25:30.086844   551 solver.cpp:237]     Train net output #0: loss = 0.224091 (* 1 = 0.224091 loss)\n",
      "I0608 01:25:30.086859   551 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565\n",
      "I0608 01:25:30.408408   551 solver.cpp:218] Iteration 200 (310.996 iter/s, 0.321547s/100 iters), loss = 0.168978\n",
      "I0608 01:25:30.408468   551 solver.cpp:237]     Train net output #0: loss = 0.168978 (* 1 = 0.168978 loss)\n",
      "I0608 01:25:30.408483   551 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258\n",
      "I0608 01:25:30.707087   551 solver.cpp:218] Iteration 300 (334.89 iter/s, 0.298606s/100 iters), loss = 0.142814\n",
      "I0608 01:25:30.707149   551 solver.cpp:237]     Train net output #0: loss = 0.142814 (* 1 = 0.142814 loss)\n",
      "I0608 01:25:30.707195   551 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075\n",
      "I0608 01:25:31.005627   551 solver.cpp:218] Iteration 400 (335.044 iter/s, 0.298468s/100 iters), loss = 0.0835249\n",
      "I0608 01:25:31.005686   551 solver.cpp:237]     Train net output #0: loss = 0.0835248 (* 1 = 0.0835248 loss)\n",
      "I0608 01:25:31.005702   551 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013\n",
      "I0608 01:25:31.303761   551 solver.cpp:330] Iteration 500, Testing net (#0)\n",
      "I0608 01:25:31.452939   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:31.458853   551 solver.cpp:397]     Test net output #0: accuracy = 0.9716\n",
      "I0608 01:25:31.458899   551 solver.cpp:397]     Test net output #1: loss = 0.0881166 (* 1 = 0.0881166 loss)\n",
      "I0608 01:25:31.461632   551 solver.cpp:218] Iteration 500 (219.333 iter/s, 0.455928s/100 iters), loss = 0.0948226\n",
      "I0608 01:25:31.461673   551 solver.cpp:237]     Train net output #0: loss = 0.0948226 (* 1 = 0.0948226 loss)\n",
      "I0608 01:25:31.461689   551 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069\n",
      "I0608 01:25:31.758884   551 solver.cpp:218] Iteration 600 (336.475 iter/s, 0.297199s/100 iters), loss = 0.077807\n",
      "I0608 01:25:31.758942   551 solver.cpp:237]     Train net output #0: loss = 0.077807 (* 1 = 0.077807 loss)\n",
      "I0608 01:25:31.758956   551 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724\n",
      "I0608 01:25:32.069435   551 solver.cpp:218] Iteration 700 (322.079 iter/s, 0.310482s/100 iters), loss = 0.12625\n",
      "I0608 01:25:32.069490   551 solver.cpp:237]     Train net output #0: loss = 0.12625 (* 1 = 0.12625 loss)\n",
      "I0608 01:25:32.069504   551 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522\n",
      "I0608 01:25:32.367339   551 solver.cpp:218] Iteration 800 (335.753 iter/s, 0.297838s/100 iters), loss = 0.21664\n",
      "I0608 01:25:32.367399   551 solver.cpp:237]     Train net output #0: loss = 0.21664 (* 1 = 0.21664 loss)\n",
      "I0608 01:25:32.367414   551 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913\n",
      "I0608 01:25:32.666213   551 solver.cpp:218] Iteration 900 (334.668 iter/s, 0.298804s/100 iters), loss = 0.156391\n",
      "I0608 01:25:32.677788   551 solver.cpp:237]     Train net output #0: loss = 0.156391 (* 1 = 0.156391 loss)\n",
      "I0608 01:25:32.689196   551 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411\n",
      "I0608 01:25:32.800560   555 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:32.807832   551 blocking_queue.cpp:49] Waiting for data\n",
      "I0608 01:25:33.018625   551 solver.cpp:330] Iteration 1000, Testing net (#0)\n",
      "I0608 01:25:33.179425   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:33.193761   551 solver.cpp:397]     Test net output #0: accuracy = 0.9815\n",
      "I0608 01:25:33.204074   551 solver.cpp:397]     Test net output #1: loss = 0.0610212 (* 1 = 0.0610212 loss)\n",
      "I0608 01:25:33.217787   551 solver.cpp:218] Iteration 1000 (185.184 iter/s, 0.540004s/100 iters), loss = 0.0999852\n",
      "I0608 01:25:33.230378   551 solver.cpp:237]     Train net output #0: loss = 0.0999852 (* 1 = 0.0999852 loss)\n",
      "I0608 01:25:33.241209   551 sgd_solver.cpp:105] Iteration 1000, lr = 0.00931012\n",
      "I0608 01:25:33.566154   551 solver.cpp:218] Iteration 1100 (297.819 iter/s, 0.335774s/100 iters), loss = 0.00737412\n",
      "I0608 01:25:33.579432   551 solver.cpp:237]     Train net output #0: loss = 0.00737414 (* 1 = 0.00737414 loss)\n",
      "I0608 01:25:33.595589   551 sgd_solver.cpp:105] Iteration 1100, lr = 0.00924715\n",
      "I0608 01:25:33.906249   551 solver.cpp:218] Iteration 1200 (305.977 iter/s, 0.326822s/100 iters), loss = 0.0138592\n",
      "I0608 01:25:33.918673   551 solver.cpp:237]     Train net output #0: loss = 0.0138592 (* 1 = 0.0138592 loss)\n",
      "I0608 01:25:33.930476   551 sgd_solver.cpp:105] Iteration 1200, lr = 0.00918515\n",
      "I0608 01:25:34.244587   551 solver.cpp:218] Iteration 1300 (306.831 iter/s, 0.325912s/100 iters), loss = 0.0164538\n",
      "I0608 01:25:34.256598   551 solver.cpp:237]     Train net output #0: loss = 0.0164538 (* 1 = 0.0164538 loss)\n",
      "I0608 01:25:34.266922   551 sgd_solver.cpp:105] Iteration 1300, lr = 0.00912412\n",
      "I0608 01:25:34.582427   551 solver.cpp:218] Iteration 1400 (306.902 iter/s, 0.325837s/100 iters), loss = 0.0099988\n",
      "I0608 01:25:34.594357   551 solver.cpp:237]     Train net output #0: loss = 0.00999885 (* 1 = 0.00999885 loss)\n",
      "I0608 01:25:34.605096   551 sgd_solver.cpp:105] Iteration 1400, lr = 0.00906403\n",
      "I0608 01:25:34.911739   551 solver.cpp:330] Iteration 1500, Testing net (#0)\n",
      "I0608 01:25:35.070503   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:35.085234   551 solver.cpp:397]     Test net output #0: accuracy = 0.9851\n",
      "I0608 01:25:35.095971   551 solver.cpp:397]     Test net output #1: loss = 0.0471886 (* 1 = 0.0471886 loss)\n",
      "I0608 01:25:35.109707   551 solver.cpp:218] Iteration 1500 (194.039 iter/s, 0.51536s/100 iters), loss = 0.0784236\n",
      "I0608 01:25:35.122519   551 solver.cpp:237]     Train net output #0: loss = 0.0784237 (* 1 = 0.0784237 loss)\n",
      "I0608 01:25:35.132804   551 sgd_solver.cpp:105] Iteration 1500, lr = 0.00900485\n",
      "I0608 01:25:35.442507   551 solver.cpp:218] Iteration 1600 (312.512 iter/s, 0.319988s/100 iters), loss = 0.120206\n",
      "I0608 01:25:35.456001   551 solver.cpp:237]     Train net output #0: loss = 0.120206 (* 1 = 0.120206 loss)\n",
      "I0608 01:25:35.469151   551 sgd_solver.cpp:105] Iteration 1600, lr = 0.00894657\n",
      "I0608 01:25:35.781903   551 solver.cpp:218] Iteration 1700 (306.84 iter/s, 0.325903s/100 iters), loss = 0.0268717\n",
      "I0608 01:25:35.793326   551 solver.cpp:237]     Train net output #0: loss = 0.0268718 (* 1 = 0.0268718 loss)\n",
      "I0608 01:25:35.803292   551 sgd_solver.cpp:105] Iteration 1700, lr = 0.00888916\n",
      "I0608 01:25:36.112888   551 solver.cpp:218] Iteration 1800 (312.942 iter/s, 0.319548s/100 iters), loss = 0.0218889\n",
      "I0608 01:25:36.125576   551 solver.cpp:237]     Train net output #0: loss = 0.021889 (* 1 = 0.021889 loss)\n",
      "I0608 01:25:36.137408   551 sgd_solver.cpp:105] Iteration 1800, lr = 0.0088326\n",
      "I0608 01:25:36.357756   555 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:36.450608   551 solver.cpp:218] Iteration 1900 (307.661 iter/s, 0.325033s/100 iters), loss = 0.144758\n",
      "I0608 01:25:36.476111   551 solver.cpp:237]     Train net output #0: loss = 0.144758 (* 1 = 0.144758 loss)\n",
      "I0608 01:25:36.487946   551 sgd_solver.cpp:105] Iteration 1900, lr = 0.00877687\n",
      "I0608 01:25:36.793296   551 solver.cpp:330] Iteration 2000, Testing net (#0)\n",
      "I0608 01:25:36.956841   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:36.970914   551 solver.cpp:397]     Test net output #0: accuracy = 0.9842\n",
      "I0608 01:25:36.997138   551 solver.cpp:397]     Test net output #1: loss = 0.0460443 (* 1 = 0.0460443 loss)\n",
      "I0608 01:25:37.010998   551 solver.cpp:218] Iteration 2000 (186.953 iter/s, 0.534893s/100 iters), loss = 0.0106751\n",
      "I0608 01:25:37.023488   551 solver.cpp:237]     Train net output #0: loss = 0.0106752 (* 1 = 0.0106752 loss)\n",
      "I0608 01:25:37.034981   551 sgd_solver.cpp:105] Iteration 2000, lr = 0.00872196\n",
      "I0608 01:25:37.345544   551 solver.cpp:218] Iteration 2100 (310.51 iter/s, 0.322051s/100 iters), loss = 0.0328515\n",
      "I0608 01:25:37.357681   551 solver.cpp:237]     Train net output #0: loss = 0.0328516 (* 1 = 0.0328516 loss)\n",
      "I0608 01:25:37.368613   551 sgd_solver.cpp:105] Iteration 2100, lr = 0.00866784\n",
      "I0608 01:25:37.681907   551 solver.cpp:218] Iteration 2200 (308.419 iter/s, 0.324234s/100 iters), loss = 0.00987182\n",
      "I0608 01:25:37.695160   551 solver.cpp:237]     Train net output #0: loss = 0.00987191 (* 1 = 0.00987191 loss)\n",
      "I0608 01:25:37.704847   551 sgd_solver.cpp:105] Iteration 2200, lr = 0.0086145\n",
      "I0608 01:25:38.021963   551 solver.cpp:218] Iteration 2300 (305.99 iter/s, 0.326808s/100 iters), loss = 0.0889982\n",
      "I0608 01:25:38.033530   551 solver.cpp:237]     Train net output #0: loss = 0.0889983 (* 1 = 0.0889983 loss)\n",
      "I0608 01:25:38.044127   551 sgd_solver.cpp:105] Iteration 2300, lr = 0.00856192\n",
      "I0608 01:25:38.363425   551 solver.cpp:218] Iteration 2400 (303.122 iter/s, 0.3299s/100 iters), loss = 0.00824029\n",
      "I0608 01:25:38.376024   551 solver.cpp:237]     Train net output #0: loss = 0.00824041 (* 1 = 0.00824041 loss)\n",
      "I0608 01:25:38.387521   551 sgd_solver.cpp:105] Iteration 2400, lr = 0.00851008\n",
      "I0608 01:25:38.715096   551 solver.cpp:330] Iteration 2500, Testing net (#0)\n",
      "I0608 01:25:38.876413   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:38.890288   551 solver.cpp:397]     Test net output #0: accuracy = 0.9838\n",
      "I0608 01:25:38.901330   551 solver.cpp:397]     Test net output #1: loss = 0.0509366 (* 1 = 0.0509366 loss)\n",
      "I0608 01:25:38.917234   551 solver.cpp:218] Iteration 2500 (185.474 iter/s, 0.53916s/100 iters), loss = 0.0183532\n",
      "I0608 01:25:38.931082   551 solver.cpp:237]     Train net output #0: loss = 0.0183534 (* 1 = 0.0183534 loss)\n",
      "I0608 01:25:38.948209   551 sgd_solver.cpp:105] Iteration 2500, lr = 0.00845897\n",
      "I0608 01:25:39.263054   551 solver.cpp:218] Iteration 2600 (301.235 iter/s, 0.331967s/100 iters), loss = 0.0428019\n",
      "I0608 01:25:39.275180   551 solver.cpp:237]     Train net output #0: loss = 0.042802 (* 1 = 0.042802 loss)\n",
      "I0608 01:25:39.299518   551 sgd_solver.cpp:105] Iteration 2600, lr = 0.00840857\n",
      "I0608 01:25:39.610595   551 solver.cpp:218] Iteration 2700 (298.135 iter/s, 0.335419s/100 iters), loss = 0.0543665\n",
      "I0608 01:25:39.622442   551 solver.cpp:237]     Train net output #0: loss = 0.0543667 (* 1 = 0.0543667 loss)\n",
      "I0608 01:25:39.634212   551 sgd_solver.cpp:105] Iteration 2700, lr = 0.00835886\n",
      "I0608 01:25:39.946391   551 solver.cpp:218] Iteration 2800 (308.693 iter/s, 0.323947s/100 iters), loss = 0.00208909\n",
      "I0608 01:25:39.958773   551 solver.cpp:237]     Train net output #0: loss = 0.00208923 (* 1 = 0.00208923 loss)\n",
      "I0608 01:25:39.969036   551 sgd_solver.cpp:105] Iteration 2800, lr = 0.00830984\n",
      "I0608 01:25:40.005761   555 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:40.290240   551 solver.cpp:218] Iteration 2900 (301.679 iter/s, 0.331478s/100 iters), loss = 0.0133733\n",
      "I0608 01:25:40.302958   551 solver.cpp:237]     Train net output #0: loss = 0.0133734 (* 1 = 0.0133734 loss)\n",
      "I0608 01:25:40.313990   551 sgd_solver.cpp:105] Iteration 2900, lr = 0.00826148\n",
      "I0608 01:25:40.620383   551 solver.cpp:330] Iteration 3000, Testing net (#0)\n",
      "I0608 01:25:40.790940   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:40.806473   551 solver.cpp:397]     Test net output #0: accuracy = 0.9881\n",
      "I0608 01:25:40.816967   551 solver.cpp:397]     Test net output #1: loss = 0.037105 (* 1 = 0.037105 loss)\n",
      "I0608 01:25:40.830684   551 solver.cpp:218] Iteration 3000 (189.488 iter/s, 0.527739s/100 iters), loss = 0.0084786\n",
      "I0608 01:25:40.843295   551 solver.cpp:237]     Train net output #0: loss = 0.00847876 (* 1 = 0.00847876 loss)\n",
      "I0608 01:25:40.855463   551 sgd_solver.cpp:105] Iteration 3000, lr = 0.00821377\n",
      "I0608 01:25:41.169728   551 solver.cpp:218] Iteration 3100 (306.343 iter/s, 0.326432s/100 iters), loss = 0.0166608\n",
      "I0608 01:25:41.181062   551 solver.cpp:237]     Train net output #0: loss = 0.016661 (* 1 = 0.016661 loss)\n",
      "I0608 01:25:41.192018   551 sgd_solver.cpp:105] Iteration 3100, lr = 0.0081667\n",
      "I0608 01:25:41.503048   551 solver.cpp:218] Iteration 3200 (310.581 iter/s, 0.321977s/100 iters), loss = 0.0103782\n",
      "I0608 01:25:41.515117   551 solver.cpp:237]     Train net output #0: loss = 0.0103784 (* 1 = 0.0103784 loss)\n",
      "I0608 01:25:41.525802   551 sgd_solver.cpp:105] Iteration 3200, lr = 0.00812025\n",
      "I0608 01:25:41.836094   551 solver.cpp:218] Iteration 3300 (311.548 iter/s, 0.320978s/100 iters), loss = 0.0145725\n",
      "I0608 01:25:41.848498   551 solver.cpp:237]     Train net output #0: loss = 0.0145727 (* 1 = 0.0145727 loss)\n",
      "I0608 01:25:41.858846   551 sgd_solver.cpp:105] Iteration 3300, lr = 0.00807442\n",
      "I0608 01:25:42.169533   551 solver.cpp:218] Iteration 3400 (311.492 iter/s, 0.321036s/100 iters), loss = 0.0120301\n",
      "I0608 01:25:42.182454   551 solver.cpp:237]     Train net output #0: loss = 0.0120303 (* 1 = 0.0120303 loss)\n",
      "I0608 01:25:42.193066   551 sgd_solver.cpp:105] Iteration 3400, lr = 0.00802918\n",
      "I0608 01:25:42.499462   551 solver.cpp:330] Iteration 3500, Testing net (#0)\n",
      "I0608 01:25:42.656946   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:42.671900   551 solver.cpp:397]     Test net output #0: accuracy = 0.9875\n",
      "I0608 01:25:42.682117   551 solver.cpp:397]     Test net output #1: loss = 0.0390479 (* 1 = 0.0390479 loss)\n",
      "I0608 01:25:42.694753   551 solver.cpp:218] Iteration 3500 (195.197 iter/s, 0.512304s/100 iters), loss = 0.0056886\n",
      "I0608 01:25:42.705894   551 solver.cpp:237]     Train net output #0: loss = 0.00568879 (* 1 = 0.00568879 loss)\n",
      "I0608 01:25:42.716528   551 sgd_solver.cpp:105] Iteration 3500, lr = 0.00798454\n",
      "I0608 01:25:43.027128   551 solver.cpp:218] Iteration 3600 (311.303 iter/s, 0.32123s/100 iters), loss = 0.0245971\n",
      "I0608 01:25:43.039607   551 solver.cpp:237]     Train net output #0: loss = 0.0245973 (* 1 = 0.0245973 loss)\n",
      "I0608 01:25:43.050040   551 sgd_solver.cpp:105] Iteration 3600, lr = 0.00794046\n",
      "I0608 01:25:43.359196   551 solver.cpp:218] Iteration 3700 (312.899 iter/s, 0.319592s/100 iters), loss = 0.0105918\n",
      "I0608 01:25:43.371536   551 solver.cpp:237]     Train net output #0: loss = 0.010592 (* 1 = 0.010592 loss)\n",
      "I0608 01:25:43.384488   551 sgd_solver.cpp:105] Iteration 3700, lr = 0.00789695\n",
      "I0608 01:25:43.533128   555 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:43.699759   551 solver.cpp:218] Iteration 3800 (304.673 iter/s, 0.328221s/100 iters), loss = 0.00467768\n",
      "I0608 01:25:43.715008   551 solver.cpp:237]     Train net output #0: loss = 0.00467789 (* 1 = 0.00467789 loss)\n",
      "I0608 01:25:43.725118   551 sgd_solver.cpp:105] Iteration 3800, lr = 0.007854\n",
      "I0608 01:25:44.035511   551 solver.cpp:218] Iteration 3900 (312.007 iter/s, 0.320506s/100 iters), loss = 0.0379113\n",
      "I0608 01:25:44.047351   551 solver.cpp:237]     Train net output #0: loss = 0.0379115 (* 1 = 0.0379115 loss)\n",
      "I0608 01:25:44.058027   551 sgd_solver.cpp:105] Iteration 3900, lr = 0.00781158\n",
      "I0608 01:25:44.369055   551 solver.cpp:330] Iteration 4000, Testing net (#0)\n",
      "I0608 01:25:44.529472   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:44.542892   551 solver.cpp:397]     Test net output #0: accuracy = 0.9899\n",
      "I0608 01:25:44.553879   551 solver.cpp:397]     Test net output #1: loss = 0.0307643 (* 1 = 0.0307643 loss)\n",
      "I0608 01:25:44.567266   551 solver.cpp:218] Iteration 4000 (192.337 iter/s, 0.519921s/100 iters), loss = 0.0147914\n",
      "I0608 01:25:44.579008   551 solver.cpp:237]     Train net output #0: loss = 0.0147916 (* 1 = 0.0147916 loss)\n",
      "I0608 01:25:44.590620   551 sgd_solver.cpp:105] Iteration 4000, lr = 0.0077697\n",
      "I0608 01:25:44.907047   551 solver.cpp:218] Iteration 4100 (304.84 iter/s, 0.328041s/100 iters), loss = 0.0243358\n",
      "I0608 01:25:44.919116   551 solver.cpp:237]     Train net output #0: loss = 0.024336 (* 1 = 0.024336 loss)\n",
      "I0608 01:25:44.930058   551 sgd_solver.cpp:105] Iteration 4100, lr = 0.00772833\n",
      "I0608 01:25:45.240276   551 solver.cpp:218] Iteration 4200 (311.372 iter/s, 0.321159s/100 iters), loss = 0.0067844\n",
      "I0608 01:25:45.252192   551 solver.cpp:237]     Train net output #0: loss = 0.0067846 (* 1 = 0.0067846 loss)\n",
      "I0608 01:25:45.263154   551 sgd_solver.cpp:105] Iteration 4200, lr = 0.00768748\n",
      "I0608 01:25:45.574631   551 solver.cpp:218] Iteration 4300 (310.135 iter/s, 0.32244s/100 iters), loss = 0.0718847\n",
      "I0608 01:25:45.587543   551 solver.cpp:237]     Train net output #0: loss = 0.0718849 (* 1 = 0.0718849 loss)\n",
      "I0608 01:25:45.613243   551 sgd_solver.cpp:105] Iteration 4300, lr = 0.00764712\n",
      "I0608 01:25:45.923100   551 solver.cpp:218] Iteration 4400 (297.998 iter/s, 0.335573s/100 iters), loss = 0.014158\n",
      "I0608 01:25:45.935009   551 solver.cpp:237]     Train net output #0: loss = 0.0141583 (* 1 = 0.0141583 loss)\n",
      "I0608 01:25:45.945431   551 sgd_solver.cpp:105] Iteration 4400, lr = 0.00760726\n",
      "I0608 01:25:46.256726   551 solver.cpp:330] Iteration 4500, Testing net (#0)\n",
      "I0608 01:25:46.415258   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:46.430829   551 solver.cpp:397]     Test net output #0: accuracy = 0.9881\n",
      "I0608 01:25:46.440939   551 solver.cpp:397]     Test net output #1: loss = 0.0388139 (* 1 = 0.0388139 loss)\n",
      "I0608 01:25:46.453655   551 solver.cpp:218] Iteration 4500 (192.806 iter/s, 0.518656s/100 iters), loss = 0.00655793\n",
      "I0608 01:25:46.464094   551 solver.cpp:237]     Train net output #0: loss = 0.00655814 (* 1 = 0.00655814 loss)\n",
      "I0608 01:25:46.474156   551 sgd_solver.cpp:105] Iteration 4500, lr = 0.00756788\n",
      "I0608 01:25:46.782426   551 solver.cpp:218] Iteration 4600 (314.14 iter/s, 0.318329s/100 iters), loss = 0.0304854\n",
      "I0608 01:25:46.794266   551 solver.cpp:237]     Train net output #0: loss = 0.0304856 (* 1 = 0.0304856 loss)\n",
      "I0608 01:25:46.805420   551 sgd_solver.cpp:105] Iteration 4600, lr = 0.00752897\n",
      "I0608 01:25:47.061951   555 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:47.117178   551 solver.cpp:218] Iteration 4700 (309.679 iter/s, 0.322915s/100 iters), loss = 0.00580385\n",
      "I0608 01:25:47.129673   551 solver.cpp:237]     Train net output #0: loss = 0.00580406 (* 1 = 0.00580406 loss)\n",
      "I0608 01:25:47.140619   551 sgd_solver.cpp:105] Iteration 4700, lr = 0.00749052\n",
      "I0608 01:25:47.449575   551 solver.cpp:218] Iteration 4800 (312.598 iter/s, 0.319899s/100 iters), loss = 0.0167812\n",
      "I0608 01:25:47.461408   551 solver.cpp:237]     Train net output #0: loss = 0.0167814 (* 1 = 0.0167814 loss)\n",
      "I0608 01:25:47.471588   551 sgd_solver.cpp:105] Iteration 4800, lr = 0.00745253\n",
      "I0608 01:25:47.780757   551 solver.cpp:218] Iteration 4900 (313.137 iter/s, 0.319349s/100 iters), loss = 0.00607338\n",
      "I0608 01:25:47.792825   551 solver.cpp:237]     Train net output #0: loss = 0.00607358 (* 1 = 0.00607358 loss)\n",
      "I0608 01:25:47.803447   551 sgd_solver.cpp:105] Iteration 4900, lr = 0.00741498\n",
      "I0608 01:25:48.124145   551 solver.cpp:447] Snapshotting to binary proto file /mnt/batch/tasks/shared/LS_root/jobs/pgunda/caffe_experiment/caffe_06_08_2018_012509/mounts/afs/1cba1da6-5a83-45e1-a88e-8b397eb84356/batchaitestrgnortheurope/workspaces/pgunda/experiments/caffe_experiment/jobs/caffe_06_08_2018_012509/21459a0a-cec0-4935-891e-dfd6ec1e4be0/outputs/Models/lenet_iter_5000.caffemodel\n",
      "I0608 01:25:48.572517   551 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /mnt/batch/tasks/shared/LS_root/jobs/pgunda/caffe_experiment/caffe_06_08_2018_012509/mounts/afs/1cba1da6-5a83-45e1-a88e-8b397eb84356/batchaitestrgnortheurope/workspaces/pgunda/experiments/caffe_experiment/jobs/caffe_06_08_2018_012509/21459a0a-cec0-4935-891e-dfd6ec1e4be0/outputs/Models/lenet_iter_5000.solverstate\n",
      "I0608 01:25:48.780023   551 solver.cpp:330] Iteration 5000, Testing net (#0)\n",
      "I0608 01:25:48.941135   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:48.954998   551 solver.cpp:397]     Test net output #0: accuracy = 0.9903\n",
      "I0608 01:25:48.965986   551 solver.cpp:397]     Test net output #1: loss = 0.0306363 (* 1 = 0.0306363 loss)\n",
      "I0608 01:25:48.980288   551 solver.cpp:218] Iteration 5000 (84.2122 iter/s, 1.18748s/100 iters), loss = 0.0230101\n",
      "I0608 01:25:48.990382   551 solver.cpp:237]     Train net output #0: loss = 0.0230103 (* 1 = 0.0230103 loss)\n",
      "I0608 01:25:49.000852   551 sgd_solver.cpp:105] Iteration 5000, lr = 0.00737788\n",
      "I0608 01:25:49.317534   551 solver.cpp:218] Iteration 5100 (305.674 iter/s, 0.327146s/100 iters), loss = 0.020985\n",
      "I0608 01:25:49.329825   551 solver.cpp:237]     Train net output #0: loss = 0.0209852 (* 1 = 0.0209852 loss)\n",
      "I0608 01:25:49.345993   551 sgd_solver.cpp:105] Iteration 5100, lr = 0.0073412\n",
      "I0608 01:25:49.658777   551 solver.cpp:218] Iteration 5200 (303.985 iter/s, 0.328963s/100 iters), loss = 0.00959174\n",
      "I0608 01:25:49.670578   551 solver.cpp:237]     Train net output #0: loss = 0.00959193 (* 1 = 0.00959193 loss)\n",
      "I0608 01:25:49.680898   551 sgd_solver.cpp:105] Iteration 5200, lr = 0.00730495\n",
      "I0608 01:25:49.990483   551 solver.cpp:218] Iteration 5300 (312.595 iter/s, 0.319903s/100 iters), loss = 0.00140165\n",
      "I0608 01:25:50.002537   551 solver.cpp:237]     Train net output #0: loss = 0.00140182 (* 1 = 0.00140182 loss)\n",
      "I0608 01:25:50.012964   551 sgd_solver.cpp:105] Iteration 5300, lr = 0.00726911\n",
      "I0608 01:25:50.322170   551 solver.cpp:218] Iteration 5400 (312.837 iter/s, 0.319655s/100 iters), loss = 0.00778576\n",
      "I0608 01:25:50.333142   551 solver.cpp:237]     Train net output #0: loss = 0.00778596 (* 1 = 0.00778596 loss)\n",
      "I0608 01:25:50.345064   551 sgd_solver.cpp:105] Iteration 5400, lr = 0.00723368\n",
      "I0608 01:25:50.650585   551 solver.cpp:330] Iteration 5500, Testing net (#0)\n",
      "I0608 01:25:50.811640   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:50.824820   551 solver.cpp:397]     Test net output #0: accuracy = 0.9897\n",
      "I0608 01:25:50.834916   551 solver.cpp:397]     Test net output #1: loss = 0.0325111 (* 1 = 0.0325111 loss)\n",
      "I0608 01:25:50.848335   551 solver.cpp:218] Iteration 5500 (194.099 iter/s, 0.5152s/100 iters), loss = 0.00442769\n",
      "I0608 01:25:50.859311   551 solver.cpp:237]     Train net output #0: loss = 0.00442788 (* 1 = 0.00442788 loss)\n",
      "I0608 01:25:50.870128   551 sgd_solver.cpp:105] Iteration 5500, lr = 0.00719865\n",
      "I0608 01:25:51.178131   551 solver.cpp:218] Iteration 5600 (313.659 iter/s, 0.318817s/100 iters), loss = 0.000833324\n",
      "I0608 01:25:51.190058   551 solver.cpp:237]     Train net output #0: loss = 0.000833525 (* 1 = 0.000833525 loss)\n",
      "I0608 01:25:51.200979   551 sgd_solver.cpp:105] Iteration 5600, lr = 0.00716402\n",
      "I0608 01:25:51.272511   555 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:51.513209   551 solver.cpp:218] Iteration 5700 (309.455 iter/s, 0.323149s/100 iters), loss = 0.00427748\n",
      "I0608 01:25:51.525692   551 solver.cpp:237]     Train net output #0: loss = 0.00427768 (* 1 = 0.00427768 loss)\n",
      "I0608 01:25:51.537137   551 sgd_solver.cpp:105] Iteration 5700, lr = 0.00712977\n",
      "I0608 01:25:51.844449   551 solver.cpp:218] Iteration 5800 (313.707 iter/s, 0.318769s/100 iters), loss = 0.0214768\n",
      "I0608 01:25:51.856223   551 solver.cpp:237]     Train net output #0: loss = 0.021477 (* 1 = 0.021477 loss)\n",
      "I0608 01:25:51.865878   551 sgd_solver.cpp:105] Iteration 5800, lr = 0.0070959\n",
      "I0608 01:25:52.174913   551 solver.cpp:218] Iteration 5900 (313.787 iter/s, 0.318687s/100 iters), loss = 0.00684785\n",
      "I0608 01:25:52.186317   551 solver.cpp:237]     Train net output #0: loss = 0.00684804 (* 1 = 0.00684804 loss)\n",
      "I0608 01:25:52.197245   551 sgd_solver.cpp:105] Iteration 5900, lr = 0.0070624\n",
      "I0608 01:25:52.501711   551 solver.cpp:330] Iteration 6000, Testing net (#0)\n",
      "I0608 01:25:52.662822   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:52.677273   551 solver.cpp:397]     Test net output #0: accuracy = 0.9906\n",
      "I0608 01:25:52.687981   551 solver.cpp:397]     Test net output #1: loss = 0.029895 (* 1 = 0.029895 loss)\n",
      "I0608 01:25:52.701635   551 solver.cpp:218] Iteration 6000 (194.054 iter/s, 0.51532s/100 iters), loss = 0.00451242\n",
      "I0608 01:25:52.712041   551 solver.cpp:237]     Train net output #0: loss = 0.00451261 (* 1 = 0.00451261 loss)\n",
      "I0608 01:25:52.721895   551 sgd_solver.cpp:105] Iteration 6000, lr = 0.00702927\n",
      "I0608 01:25:53.048053   551 solver.cpp:218] Iteration 6100 (297.61 iter/s, 0.33601s/100 iters), loss = 0.00190371\n",
      "I0608 01:25:53.061040   551 solver.cpp:237]     Train net output #0: loss = 0.0019039 (* 1 = 0.0019039 loss)\n",
      "I0608 01:25:53.072639   551 sgd_solver.cpp:105] Iteration 6100, lr = 0.0069965\n",
      "I0608 01:25:53.404764   551 solver.cpp:218] Iteration 6200 (290.925 iter/s, 0.343731s/100 iters), loss = 0.00602743\n",
      "I0608 01:25:53.417230   551 solver.cpp:237]     Train net output #0: loss = 0.00602761 (* 1 = 0.00602761 loss)\n",
      "I0608 01:25:53.428189   551 sgd_solver.cpp:105] Iteration 6200, lr = 0.00696408\n",
      "I0608 01:25:53.756696   551 solver.cpp:218] Iteration 6300 (294.575 iter/s, 0.339472s/100 iters), loss = 0.00831253\n",
      "I0608 01:25:53.769302   551 solver.cpp:237]     Train net output #0: loss = 0.0083127 (* 1 = 0.0083127 loss)\n",
      "I0608 01:25:53.780019   551 sgd_solver.cpp:105] Iteration 6300, lr = 0.00693201\n",
      "I0608 01:25:54.102727   551 solver.cpp:218] Iteration 6400 (299.919 iter/s, 0.333424s/100 iters), loss = 0.00656195\n",
      "I0608 01:25:54.114475   551 solver.cpp:237]     Train net output #0: loss = 0.00656213 (* 1 = 0.00656213 loss)\n",
      "I0608 01:25:54.124655   551 sgd_solver.cpp:105] Iteration 6400, lr = 0.00690029\n",
      "I0608 01:25:54.436151   551 solver.cpp:330] Iteration 6500, Testing net (#0)\n",
      "I0608 01:25:54.596011   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:54.609956   551 solver.cpp:397]     Test net output #0: accuracy = 0.9913\n",
      "I0608 01:25:54.620957   551 solver.cpp:397]     Test net output #1: loss = 0.0310539 (* 1 = 0.0310539 loss)\n",
      "I0608 01:25:54.633958   551 solver.cpp:218] Iteration 6500 (192.495 iter/s, 0.519493s/100 iters), loss = 0.0114544\n",
      "I0608 01:25:54.658282   551 solver.cpp:237]     Train net output #0: loss = 0.0114546 (* 1 = 0.0114546 loss)\n",
      "I0608 01:25:54.668889   551 sgd_solver.cpp:105] Iteration 6500, lr = 0.0068689\n",
      "I0608 01:25:54.853457   555 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:54.982733   551 solver.cpp:218] Iteration 6600 (308.219 iter/s, 0.324444s/100 iters), loss = 0.0320335\n",
      "I0608 01:25:54.994196   551 solver.cpp:237]     Train net output #0: loss = 0.0320336 (* 1 = 0.0320336 loss)\n",
      "I0608 01:25:55.004024   551 sgd_solver.cpp:105] Iteration 6600, lr = 0.00683784\n",
      "I0608 01:25:55.322942   551 solver.cpp:218] Iteration 6700 (304.18 iter/s, 0.328753s/100 iters), loss = 0.00907711\n",
      "I0608 01:25:55.345105   551 solver.cpp:237]     Train net output #0: loss = 0.00907728 (* 1 = 0.00907728 loss)\n",
      "I0608 01:25:55.356086   551 sgd_solver.cpp:105] Iteration 6700, lr = 0.00680711\n",
      "I0608 01:25:55.672827   551 solver.cpp:218] Iteration 6800 (305.131 iter/s, 0.327728s/100 iters), loss = 0.00405104\n",
      "I0608 01:25:55.691042   551 solver.cpp:237]     Train net output #0: loss = 0.0040512 (* 1 = 0.0040512 loss)\n",
      "I0608 01:25:55.702105   551 sgd_solver.cpp:105] Iteration 6800, lr = 0.0067767\n",
      "I0608 01:25:56.012524   551 solver.cpp:218] Iteration 6900 (311.062 iter/s, 0.32148s/100 iters), loss = 0.00834504\n",
      "I0608 01:25:56.025470   551 solver.cpp:237]     Train net output #0: loss = 0.00834521 (* 1 = 0.00834521 loss)\n",
      "I0608 01:25:56.037479   551 sgd_solver.cpp:105] Iteration 6900, lr = 0.0067466\n",
      "I0608 01:25:56.348279   551 solver.cpp:330] Iteration 7000, Testing net (#0)\n",
      "I0608 01:25:56.505393   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:56.519080   551 solver.cpp:397]     Test net output #0: accuracy = 0.9893\n",
      "I0608 01:25:56.529925   551 solver.cpp:397]     Test net output #1: loss = 0.0324943 (* 1 = 0.0324943 loss)\n",
      "I0608 01:25:56.543756   551 solver.cpp:218] Iteration 7000 (192.942 iter/s, 0.518291s/100 iters), loss = 0.00997321\n",
      "I0608 01:25:56.554265   551 solver.cpp:237]     Train net output #0: loss = 0.00997337 (* 1 = 0.00997337 loss)\n",
      "I0608 01:25:56.564832   551 sgd_solver.cpp:105] Iteration 7000, lr = 0.00671681\n",
      "I0608 01:25:56.873172   551 solver.cpp:218] Iteration 7100 (313.572 iter/s, 0.318906s/100 iters), loss = 0.0123644\n",
      "I0608 01:25:56.886101   551 solver.cpp:237]     Train net output #0: loss = 0.0123645 (* 1 = 0.0123645 loss)\n",
      "I0608 01:25:56.899132   551 sgd_solver.cpp:105] Iteration 7100, lr = 0.00668733\n",
      "I0608 01:25:57.210573   551 solver.cpp:218] Iteration 7200 (308.195 iter/s, 0.32447s/100 iters), loss = 0.00506109\n",
      "I0608 01:25:57.222293   551 solver.cpp:237]     Train net output #0: loss = 0.00506126 (* 1 = 0.00506126 loss)\n",
      "I0608 01:25:57.233968   551 sgd_solver.cpp:105] Iteration 7200, lr = 0.00665815\n",
      "I0608 01:25:57.544093   551 solver.cpp:218] Iteration 7300 (310.747 iter/s, 0.321805s/100 iters), loss = 0.0164116\n",
      "I0608 01:25:57.556088   551 solver.cpp:237]     Train net output #0: loss = 0.0164118 (* 1 = 0.0164118 loss)\n",
      "I0608 01:25:57.566720   551 sgd_solver.cpp:105] Iteration 7300, lr = 0.00662927\n",
      "I0608 01:25:57.877413   551 solver.cpp:218] Iteration 7400 (311.214 iter/s, 0.321322s/100 iters), loss = 0.00405223\n",
      "I0608 01:25:57.889482   551 solver.cpp:237]     Train net output #0: loss = 0.00405238 (* 1 = 0.00405238 loss)\n",
      "I0608 01:25:57.900821   551 sgd_solver.cpp:105] Iteration 7400, lr = 0.00660067\n",
      "I0608 01:25:58.197474   555 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:58.211588   551 solver.cpp:330] Iteration 7500, Testing net (#0)\n",
      "I0608 01:25:58.383278   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:25:58.397068   551 solver.cpp:397]     Test net output #0: accuracy = 0.9911\n",
      "I0608 01:25:58.407893   551 solver.cpp:397]     Test net output #1: loss = 0.0328479 (* 1 = 0.0328479 loss)\n",
      "I0608 01:25:58.420648   551 solver.cpp:218] Iteration 7500 (188.263 iter/s, 0.531172s/100 iters), loss = 0.00219295\n",
      "I0608 01:25:58.432129   551 solver.cpp:237]     Train net output #0: loss = 0.0021931 (* 1 = 0.0021931 loss)\n",
      "I0608 01:25:58.442884   551 sgd_solver.cpp:105] Iteration 7500, lr = 0.00657236\n",
      "I0608 01:25:58.750943   551 solver.cpp:218] Iteration 7600 (313.672 iter/s, 0.318804s/100 iters), loss = 0.00487435\n",
      "I0608 01:25:58.762164   551 solver.cpp:237]     Train net output #0: loss = 0.0048745 (* 1 = 0.0048745 loss)\n",
      "I0608 01:25:58.773329   551 sgd_solver.cpp:105] Iteration 7600, lr = 0.00654433\n",
      "I0608 01:25:59.086007   551 solver.cpp:218] Iteration 7700 (308.792 iter/s, 0.323842s/100 iters), loss = 0.0219795\n",
      "I0608 01:25:59.097157   551 solver.cpp:237]     Train net output #0: loss = 0.0219796 (* 1 = 0.0219796 loss)\n",
      "I0608 01:25:59.107789   551 sgd_solver.cpp:105] Iteration 7700, lr = 0.00651658\n",
      "I0608 01:25:59.419883   551 solver.cpp:218] Iteration 7800 (309.847 iter/s, 0.32274s/100 iters), loss = 0.00480423\n",
      "I0608 01:25:59.431802   551 solver.cpp:237]     Train net output #0: loss = 0.00480438 (* 1 = 0.00480438 loss)\n",
      "I0608 01:25:59.442168   551 sgd_solver.cpp:105] Iteration 7800, lr = 0.00648911\n",
      "I0608 01:25:59.760502   551 solver.cpp:218] Iteration 7900 (304.234 iter/s, 0.328694s/100 iters), loss = 0.00287861\n",
      "I0608 01:25:59.777465   551 solver.cpp:237]     Train net output #0: loss = 0.00287876 (* 1 = 0.00287876 loss)\n",
      "I0608 01:25:59.788017   551 sgd_solver.cpp:105] Iteration 7900, lr = 0.0064619\n",
      "I0608 01:26:00.099951   551 solver.cpp:330] Iteration 8000, Testing net (#0)\n",
      "I0608 01:26:00.302456   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:26:00.317795   551 solver.cpp:397]     Test net output #0: accuracy = 0.9905\n",
      "I0608 01:26:00.328827   551 solver.cpp:397]     Test net output #1: loss = 0.0317498 (* 1 = 0.0317498 loss)\n",
      "I0608 01:26:00.342556   551 solver.cpp:218] Iteration 8000 (176.961 iter/s, 0.565095s/100 iters), loss = 0.00346178\n",
      "I0608 01:26:00.353904   551 solver.cpp:237]     Train net output #0: loss = 0.00346194 (* 1 = 0.00346194 loss)\n",
      "I0608 01:26:00.365182   551 sgd_solver.cpp:105] Iteration 8000, lr = 0.00643496\n",
      "I0608 01:26:00.674118   551 solver.cpp:218] Iteration 8100 (312.295 iter/s, 0.32021s/100 iters), loss = 0.0110642\n",
      "I0608 01:26:00.701057   551 solver.cpp:237]     Train net output #0: loss = 0.0110643 (* 1 = 0.0110643 loss)\n",
      "I0608 01:26:00.711124   551 sgd_solver.cpp:105] Iteration 8100, lr = 0.00640827\n",
      "I0608 01:26:01.019392   551 solver.cpp:218] Iteration 8200 (314.131 iter/s, 0.318338s/100 iters), loss = 0.00719826\n",
      "I0608 01:26:01.031146   551 solver.cpp:237]     Train net output #0: loss = 0.00719842 (* 1 = 0.00719842 loss)\n",
      "I0608 01:26:01.043768   551 sgd_solver.cpp:105] Iteration 8200, lr = 0.00638185\n",
      "I0608 01:26:01.359843   551 solver.cpp:218] Iteration 8300 (304.232 iter/s, 0.328696s/100 iters), loss = 0.0384692\n",
      "I0608 01:26:01.371193   551 solver.cpp:237]     Train net output #0: loss = 0.0384694 (* 1 = 0.0384694 loss)\n",
      "I0608 01:26:01.382280   551 sgd_solver.cpp:105] Iteration 8300, lr = 0.00635567\n",
      "I0608 01:26:01.697787   551 solver.cpp:218] Iteration 8400 (306.191 iter/s, 0.326593s/100 iters), loss = 0.0085139\n",
      "I0608 01:26:01.710002   551 solver.cpp:237]     Train net output #0: loss = 0.00851406 (* 1 = 0.00851406 loss)\n",
      "I0608 01:26:01.719677   551 sgd_solver.cpp:105] Iteration 8400, lr = 0.00632975\n",
      "I0608 01:26:01.828822   555 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:26:02.028800   551 solver.cpp:330] Iteration 8500, Testing net (#0)\n",
      "I0608 01:26:02.211602   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:26:02.225057   551 solver.cpp:397]     Test net output #0: accuracy = 0.9909\n",
      "I0608 01:26:02.234807   551 solver.cpp:397]     Test net output #1: loss = 0.0301333 (* 1 = 0.0301333 loss)\n",
      "I0608 01:26:02.248704   551 solver.cpp:218] Iteration 8500 (185.629 iter/s, 0.538709s/100 iters), loss = 0.00482449\n",
      "I0608 01:26:02.259112   551 solver.cpp:237]     Train net output #0: loss = 0.00482464 (* 1 = 0.00482464 loss)\n",
      "I0608 01:26:02.268739   551 sgd_solver.cpp:105] Iteration 8500, lr = 0.00630407\n",
      "I0608 01:26:02.579872   551 solver.cpp:218] Iteration 8600 (311.767 iter/s, 0.320753s/100 iters), loss = 0.000820354\n",
      "I0608 01:26:02.592046   551 solver.cpp:237]     Train net output #0: loss = 0.000820492 (* 1 = 0.000820492 loss)\n",
      "I0608 01:26:02.602214   551 sgd_solver.cpp:105] Iteration 8600, lr = 0.00627864\n",
      "I0608 01:26:02.913563   551 solver.cpp:218] Iteration 8700 (311.026 iter/s, 0.321517s/100 iters), loss = 0.00376285\n",
      "I0608 01:26:02.925637   551 solver.cpp:237]     Train net output #0: loss = 0.00376299 (* 1 = 0.00376299 loss)\n",
      "I0608 01:26:02.936887   551 sgd_solver.cpp:105] Iteration 8700, lr = 0.00625344\n",
      "I0608 01:26:03.248989   551 solver.cpp:218] Iteration 8800 (309.258 iter/s, 0.323355s/100 iters), loss = 0.000987826\n",
      "I0608 01:26:03.261086   551 solver.cpp:237]     Train net output #0: loss = 0.000987968 (* 1 = 0.000987968 loss)\n",
      "I0608 01:26:03.270920   551 sgd_solver.cpp:105] Iteration 8800, lr = 0.00622847\n",
      "I0608 01:26:03.580701   551 solver.cpp:218] Iteration 8900 (312.881 iter/s, 0.31961s/100 iters), loss = 0.000616283\n",
      "I0608 01:26:03.593205   551 solver.cpp:237]     Train net output #0: loss = 0.000616424 (* 1 = 0.000616424 loss)\n",
      "I0608 01:26:03.604313   551 sgd_solver.cpp:105] Iteration 8900, lr = 0.00620374\n",
      "I0608 01:26:03.910802   551 solver.cpp:330] Iteration 9000, Testing net (#0)\n",
      "I0608 01:26:04.079701   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:26:04.095325   551 solver.cpp:397]     Test net output #0: accuracy = 0.9912\n",
      "I0608 01:26:04.111590   551 solver.cpp:397]     Test net output #1: loss = 0.0304925 (* 1 = 0.0304925 loss)\n",
      "I0608 01:26:04.136204   551 solver.cpp:218] Iteration 9000 (184.162 iter/s, 0.542999s/100 iters), loss = 0.017033\n",
      "I0608 01:26:04.146725   551 solver.cpp:237]     Train net output #0: loss = 0.0170332 (* 1 = 0.0170332 loss)\n",
      "I0608 01:26:04.158203   551 sgd_solver.cpp:105] Iteration 9000, lr = 0.00617924\n",
      "I0608 01:26:04.508651   551 solver.cpp:218] Iteration 9100 (276.279 iter/s, 0.361953s/100 iters), loss = 0.00873713\n",
      "I0608 01:26:04.521188   551 solver.cpp:237]     Train net output #0: loss = 0.00873727 (* 1 = 0.00873727 loss)\n",
      "I0608 01:26:04.531909   551 sgd_solver.cpp:105] Iteration 9100, lr = 0.00615496\n",
      "I0608 01:26:04.842182   551 solver.cpp:218] Iteration 9200 (311.526 iter/s, 0.321s/100 iters), loss = 0.0033444\n",
      "I0608 01:26:04.853534   551 solver.cpp:237]     Train net output #0: loss = 0.00334454 (* 1 = 0.00334454 loss)\n",
      "I0608 01:26:04.864843   551 sgd_solver.cpp:105] Iteration 9200, lr = 0.0061309\n",
      "I0608 01:26:05.176563   551 solver.cpp:218] Iteration 9300 (309.562 iter/s, 0.323038s/100 iters), loss = 0.00802708\n",
      "I0608 01:26:05.187952   551 solver.cpp:237]     Train net output #0: loss = 0.00802722 (* 1 = 0.00802722 loss)\n",
      "I0608 01:26:05.199684   551 sgd_solver.cpp:105] Iteration 9300, lr = 0.00610706\n",
      "I0608 01:26:05.422255   555 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:26:05.518477   551 solver.cpp:218] Iteration 9400 (302.55 iter/s, 0.330524s/100 iters), loss = 0.0439088\n",
      "I0608 01:26:05.531234   551 solver.cpp:237]     Train net output #0: loss = 0.043909 (* 1 = 0.043909 loss)\n",
      "I0608 01:26:05.542017   551 sgd_solver.cpp:105] Iteration 9400, lr = 0.00608343\n",
      "I0608 01:26:05.845166   551 solver.cpp:330] Iteration 9500, Testing net (#0)\n",
      "I0608 01:26:06.017943   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:26:06.031713   551 solver.cpp:397]     Test net output #0: accuracy = 0.9891\n",
      "I0608 01:26:06.042901   551 solver.cpp:397]     Test net output #1: loss = 0.0344574 (* 1 = 0.0344574 loss)\n",
      "I0608 01:26:06.055688   551 solver.cpp:218] Iteration 9500 (190.673 iter/s, 0.524458s/100 iters), loss = 0.00598791\n",
      "I0608 01:26:06.065793   551 solver.cpp:237]     Train net output #0: loss = 0.00598806 (* 1 = 0.00598806 loss)\n",
      "I0608 01:26:06.075605   551 sgd_solver.cpp:105] Iteration 9500, lr = 0.00606002\n",
      "I0608 01:26:06.383690   551 solver.cpp:218] Iteration 9600 (314.564 iter/s, 0.3179s/100 iters), loss = 0.00276058\n",
      "I0608 01:26:06.395269   551 solver.cpp:237]     Train net output #0: loss = 0.00276073 (* 1 = 0.00276073 loss)\n",
      "I0608 01:26:06.405467   551 sgd_solver.cpp:105] Iteration 9600, lr = 0.00603682\n",
      "I0608 01:26:06.713615   551 solver.cpp:218] Iteration 9700 (314.279 iter/s, 0.318188s/100 iters), loss = 0.00248658\n",
      "I0608 01:26:06.734567   551 solver.cpp:237]     Train net output #0: loss = 0.00248674 (* 1 = 0.00248674 loss)\n",
      "I0608 01:26:06.744849   551 sgd_solver.cpp:105] Iteration 9700, lr = 0.00601382\n",
      "I0608 01:26:07.052345   551 solver.cpp:218] Iteration 9800 (314.687 iter/s, 0.317776s/100 iters), loss = 0.0095398\n",
      "I0608 01:26:07.064431   551 solver.cpp:237]     Train net output #0: loss = 0.00953995 (* 1 = 0.00953995 loss)\n",
      "I0608 01:26:07.075817   551 sgd_solver.cpp:105] Iteration 9800, lr = 0.00599102\n",
      "I0608 01:26:07.386992   551 solver.cpp:218] Iteration 9900 (310.022 iter/s, 0.322558s/100 iters), loss = 0.00531227\n",
      "I0608 01:26:07.398833   551 solver.cpp:237]     Train net output #0: loss = 0.00531242 (* 1 = 0.00531242 loss)\n",
      "I0608 01:26:07.408756   551 sgd_solver.cpp:105] Iteration 9900, lr = 0.00596843\n",
      "I0608 01:26:07.715191   551 solver.cpp:447] Snapshotting to binary proto file /mnt/batch/tasks/shared/LS_root/jobs/pgunda/caffe_experiment/caffe_06_08_2018_012509/mounts/afs/1cba1da6-5a83-45e1-a88e-8b397eb84356/batchaitestrgnortheurope/workspaces/pgunda/experiments/caffe_experiment/jobs/caffe_06_08_2018_012509/21459a0a-cec0-4935-891e-dfd6ec1e4be0/outputs/Models/lenet_iter_10000.caffemodel\n",
      "I0608 01:26:08.140648   551 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /mnt/batch/tasks/shared/LS_root/jobs/pgunda/caffe_experiment/caffe_06_08_2018_012509/mounts/afs/1cba1da6-5a83-45e1-a88e-8b397eb84356/batchaitestrgnortheurope/workspaces/pgunda/experiments/caffe_experiment/jobs/caffe_06_08_2018_012509/21459a0a-cec0-4935-891e-dfd6ec1e4be0/outputs/Models/lenet_iter_10000.solverstate\n",
      "I0608 01:26:08.331123   551 solver.cpp:310] Iteration 10000, loss = 0.00241095\n",
      "I0608 01:26:08.342855   551 solver.cpp:330] Iteration 10000, Testing net (#0)\n",
      "I0608 01:26:08.521468   556 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0608 01:26:08.535298   551 solver.cpp:397]     Test net output #0: accuracy = 0.9913\n",
      "I0608 01:26:08.545748   551 solver.cpp:397]     Test net output #1: loss = 0.0290449 (* 1 = 0.0290449 loss)\n",
      "I0608 01:26:08.555845   551 solver.cpp:315] Optimization Done.\n",
      "I0608 01:26:08.566077   551 caffe.cpp:259] Optimization Done.\n",
      "Job state: succeeded ExitCode: 0\n"
     ]
    }
   ],
   "source": [
    "utilities.wait_for_job_completion(client, cfg.resource_group, cfg.workspace, \n",
    "                                  experiment_name, job_name, cluster_name, 'stdouterr', 'stderr.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List stdout.txt and stderr.txt files for the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://baillfqfcyapknc.file.core.windows.net/batchaisample/1cba1da6-5a83-45e1-a88e-8b397eb84356/batchaitestrgnortheurope/workspaces/pgunda/experiments/caffe_experiment/jobs/caffe_06_08_2018_012159/739affd9-f31b-4310-8855-3b02184d9533/stdouterr/execution.log?sv=2016-05-31&sr=f&sig=3plX2d1qlvvEpEXjWuYkheEdeJKOIYENR%2BKIr132jG0%3D&se=2018-06-08T02%3A23%3A10Z&sp=rl ...Done\n",
      "Downloading https://baillfqfcyapknc.file.core.windows.net/batchaisample/1cba1da6-5a83-45e1-a88e-8b397eb84356/batchaitestrgnortheurope/workspaces/pgunda/experiments/caffe_experiment/jobs/caffe_06_08_2018_012159/739affd9-f31b-4310-8855-3b02184d9533/stdouterr/stderr-job_prep.txt?sv=2016-05-31&sr=f&sig=4f0LL2LvLQ63JdreO5D7Gc4PuBaxpjSeso%2FUEHbPFB4%3D&se=2018-06-08T02%3A23%3A10Z&sp=rl ...Done\n",
      "Downloading https://baillfqfcyapknc.file.core.windows.net/batchaisample/1cba1da6-5a83-45e1-a88e-8b397eb84356/batchaitestrgnortheurope/workspaces/pgunda/experiments/caffe_experiment/jobs/caffe_06_08_2018_012159/739affd9-f31b-4310-8855-3b02184d9533/stdouterr/stdout-job_prep.txt?sv=2016-05-31&sr=f&sig=tntfJTQXASZGYNHOOqQi30RKAbNZighB2SVpCDPB5ZU%3D&se=2018-06-08T02%3A23%3A10Z&sp=rl ...Done\n",
      "All files downloaded\n"
     ]
    }
   ],
   "source": [
    "files = client.jobs.list_output_files(cfg.resource_group, cfg.workspace, experiment_name, job_name,\n",
    "                                      models.JobsListOutputFilesOptions(outputdirectoryid='stdouterr')) \n",
    "for f in list(files):\n",
    "    print(f.name, f.download_url or 'directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enumerate Model Output\n",
    "Previously we configured the job to use output directory with `ID='MODEL'` for model output. We can enumerate the output using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = client.jobs.list_output_files(cfg.resource_group, cfg.workspace, experiment_name, job_name,\n",
    "                                      models.JobsListOutputFilesOptions(outputdirectoryid='MODEL')) \n",
    "for f in list(files):\n",
    "    print(f.name, f.download_url or 'directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = client.jobs.delete(cfg.resource_group, cfg.workspace, experiment_name, job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Cluster\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the cluster using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = client.clusters.delete(cfg.resource_group, cfg.workspace, cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Delete File Share\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the file share completely with all files using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.delete_share(azure_file_share_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
